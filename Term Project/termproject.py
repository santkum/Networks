# -*- coding: utf-8 -*-
"""TermProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WfnFpU5rNgeR3jD0lSldYXfLVFag3tc2

Import packages - Pandas , Numpy
"""

import zipfile
from google.colab import drive
drive.mount('/content/drive/')
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import label_binarize,LabelEncoder,OneHotEncoder

"""# Extract Dataset"""

zip_ref = zipfile.ZipFile("/content/drive/MyDrive/kdd99/KDD.zip", 'r')
zip_ref.extractall()
zip_ref.close()

"""# Provide column names and save attacks broadly to 4 groups



"""

features = ["duration","protocol_type","service","flag","src_bytes",
    "dst_bytes","land","wrong_fragment","urgent","hot","num_failed_logins",
    "logged_in","num_compromised","root_shell","su_attempted","num_root",
    "num_file_creations","num_shells","num_access_files","num_outbound_cmds",
    "is_host_login","is_guest_login","count","srv_count","serror_rate",
    "srv_serror_rate","rerror_rate","srv_rerror_rate","same_srv_rate",
    "diff_srv_rate","srv_diff_host_rate","dst_host_count","dst_host_srv_count",
    "dst_host_same_srv_rate","dst_host_diff_srv_rate","dst_host_same_src_port_rate",
    "dst_host_srv_diff_host_rate","dst_host_serror_rate","dst_host_srv_serror_rate",
    "dst_host_rerror_rate","dst_host_srv_rerror_rate","attack_types"]


R2L=['warezmaster.','warezclient.','spy.','phf.','multihop.','imap.','guess_passwd.','ftp_write.']
U2R=['rootkit.','perl.','loadmodule.','buffer_overflow.']
DoS=['smurf.','teardrop.','back.','land.','neptune.','pod.']
Probe=['ipsweep.','nmap.','portsweep.','satan.']

# Original Code
# training_data = pd.read_csv("/content/kddcup.data_10_percent",names=features)
# training_data.shape

# Added by Santosh for testing
training_data = pd.read_csv("/content/drive/MyDrive/kdd99/kddcup.data_10_percent.csv",names=features)
training_data.shape

"""# Create class column to hold attack category of each record"""

attack_type = []

def create_class(training_data):
  for record_attack in training_data['attack_types']:
    if record_attack in R2L:
      attack_type.append("R2L")
    elif record_attack in U2R:
      attack_type.append("U2R")
    elif record_attack == "normal.":
      attack_type.append("Normal")
    elif record_attack in DoS:
      attack_type.append("DoS")
    elif record_attack in Probe:
      attack_type.append("Probe")
  return attack_type
    
training_data["class"] = create_class(training_data)
training_data.head()

"""# Remove features which have single value throughout"""

# Drops columns - num_outbound_cmds , is_host_login
for col in training_data.columns:
    if len(training_data[col].unique()) == 1:
        training_data.drop(col,inplace=True,axis=1)
training_data.shape

training_data.tail()

"""# Remove duplicate records"""

training_data.drop_duplicates(subset=None,keep='first',inplace=True)
training_data.shape

"""# Transforming Protocol_Type, Services, Flag to categorical variables"""

training_data.protocol_type = training_data.protocol_type.astype('category').cat.codes
training_data.service = training_data.service.astype('category').cat.codes
training_data.flag = training_data.flag.astype('category').cat.codes
training_data.tail()

training_data['class'].value_counts()

training_data['attack_types'].value_counts()

training_data_copy = training_data.copy()
training_data_copy['class'] = training_data_copy['class'].astype('category').cat.codes
corr_matrix = training_data_copy.corr(method='pearson')
plt.figure(figsize=(50,50))
sns.heatmap(corr_matrix, annot = True)

variable = ['duration', 'src_bytes', 'dst_bytes','land','wrong_fragment',
            'urgent','hot',	'num_failed_logins','num_compromised',	'root_shell',	'su_attempted',
            'num_root','num_file_creations','num_shells',	
            'num_access_files',	'is_guest_login','srv_count','rerror_rate',
            'srv_rerror_rate','diff_srv_rate','srv_diff_host_rate',
            'dst_host_diff_srv_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate']

# def feature_remove(df,variable):
#     for x in variable:
#       del df[x] 
#   return df

for x in variable:
      del training_data[x]
# training_data = feature_remove(training_data,variable)

training_data.shape

training_data = training_data.drop_duplicates(subset=None, keep='first',inplace=False)

training_data.shape

training_data.drop(columns="attack_types",inplace=True)
training_data.head()
# training_data['num_failed_logins'].unique()

scaler = StandardScaler()
# extract numerical attributes and scale it to have zero mean and unit variance  
cols = training_data.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(training_data.select_dtypes(include=['float64','int64']))
# turn the result back to a dataframe
sc_traindf = pd.DataFrame(sc_train, columns = cols)
sc_traindf.tail()

def target_feature(mydf):
  #mydf['label']= mydf['label'].value_counts().apply(np.log)  # log transformation
  target = mydf['class'] 
  return target

y= target_feature(training_data)
# print(y)

y.shape

def oneVSall(target):
  label_encoder = LabelEncoder()
  integer_encoded = label_encoder.fit_transform(target)
  onehot_encoder = OneHotEncoder(sparse=False,categories='auto')
  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
  my_target = onehot_encoder.fit_transform(integer_encoded)
  target_vari = pd.DataFrame(my_target, columns =['Dos','Normal', 'Probe', 'R2L','U2R'] )
  return target_vari

y =  oneVSall(y)
y

X= sc_traindf
X

columns_names = X.columns

"""# Test Data

"""

# Original Code
# corrected_data = pd.read_csv("/content/corrected",names=features)

# Code added by Santosh for testing
corrected_data = pd.read_csv("/content/drive/MyDrive/kdd99/corrected",names=features)

corrected_data.head()

R2L=['warezmaster.','warezclient.','spy.','phf.','multihop.','imap.','guess_passwd.','ftp_write.','named.','sendmail.','snmpgetattack.',
    'worm.','xlock.','xsnoop.','httptunnel.']

U2R=['rootkit.','perl.','loadmodule.','buffer_overflow.','ps.','sqlattack.','xterm.']
DoS=['smurf.','teardrop.','back.','land.','neptune.','pod.','apache2.','mailbomd.','processtable.','udpstrom']
Probe=['ipsweep.','nmap.','portsweep.','satan.','mscan','saint']

# corrected_data["class"] = create_class(corrected_data)
attack_type=[]
for i in corrected_data['attack_types']:
    if i == 'normal.':
        attack_type.append('Normal')
    elif i in R2L:
        attack_type.append('R2L')
    elif i in U2R:
        attack_type.append('U2R')
    elif i in DoS:
        attack_type.append('DoS')
    else:
        attack_type.append('Probe')
corrected_data["class"] = attack_type

corrected_data.head()

corrected_data.drop(columns=["num_outbound_cmds","is_host_login"],inplace=True)
corrected_data.shape

corrected_data.drop_duplicates(subset=None,keep='first',inplace=True)
corrected_data.shape

corrected_data.protocol_type = corrected_data.protocol_type.astype('category').cat.codes
corrected_data.service = corrected_data.service.astype('category').cat.codes
corrected_data.flag = corrected_data.flag.astype('category').cat.codes
corrected_data.tail()

corrected_data['class'].value_counts()

corrected_data['attack_types'].value_counts()

resampled_data = corrected_data[columns_names]
resampled_data.shape,X.shape

corrected_label = corrected_data['class']

resampled_data['class'] = corrected_label
resampled_data.shape

"""# Remove duplicate from dataframe"""

corrected_sample_data = resampled_data.drop_duplicates(subset=None, keep='first', inplace=False)

corrected_sample_data.shape

"""# Split dataset into 6 subsets"""

def subset_data(Data, subset):
    split = len(Data) / float(subset)
    mydata = []
    last = 0.0

    while last < len(Data):
       mydata.append(Data[int(last):int(last + split)])
       last += split
    return mydata

data1,data2,data3,data4,data5,data6,data7   =subset_data(corrected_sample_data, 7)

data1.shape,data2.shape,data3.shape,data4.shape,data5.shape,data6.shape, data7.shape

"""# One Hot Encoder"""

y1= target_feature(data1)
y2= target_feature(data2)
y3= target_feature(data3)
y4= target_feature(data4)
y5= target_feature(data5)
y6= target_feature(data6)
y7 = target_feature(data7)

y1_test =  oneVSall(y1)
y2_test =  oneVSall(y2)
y3_test =  oneVSall(y3)
y4_test =  oneVSall(y4)
y5_test =  oneVSall(y5)
y6_test =  oneVSall(y6)
y7_valid = oneVSall(y7)

y1_test.shape,y2_test.shape,y3_test.shape,y4_test.shape,y5_test.shape,y6_test.shape,y7_valid.shape

scaler = StandardScaler()
# extract numerical attributes and scale it to have zero mean and unit variance  
cols = data1.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(data1.select_dtypes(include=['float64','int64']))
# turn the result back to a dataframe
x1_test = pd.DataFrame(sc_train, columns = cols)

# extract numerical attributes and scale it to have zero mean and unit variance  
cols = data2.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(data2.select_dtypes(include=['float64','int64']))
# turn the result back to a dataframe
x2_test = pd.DataFrame(sc_train, columns = cols)

# extract numerical attributes and scale it to have zero mean and unit variance  
cols = data3.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(data3.select_dtypes(include=['float64','int64']))
# turn the result back to a dataframe
x3_test = pd.DataFrame(sc_train, columns = cols)

# extract numerical attributes and scale it to have zero mean and unit variance  
cols = data4.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(data4.select_dtypes(include=['float64','int64']))
# turn the result back to a dataframe
x4_test = pd.DataFrame(sc_train, columns = cols)

# extract numerical attributes and scale it to have zero mean and unit variance  
cols = data5.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(data5.select_dtypes(include=['float64','int64']))
# turn the result back to a dataframe
x5_test = pd.DataFrame(sc_train, columns = cols)

# extract numerical attributes and scale it to have zero mean and unit variance  
cols = data6.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(data6.select_dtypes(include=['float64','int64']))
# turn the result back to a dataframe
x6_test = pd.DataFrame(sc_train, columns = cols)

# extract numerical attributes and scale it to have zero mean and unit variance  
cols = data7.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(data7.select_dtypes(include=['float64','int64']))
# turn the result back to a dataframe
x7_valid = pd.DataFrame(sc_train, columns = cols)

x1_test.shape,x2_test.shape,x3_test.shape,x4_test.shape,x5_test.shape,x6_test.shape,x7_valid.shape

"""SVM Model

"""

from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC

"""**train and predict**"""

# train the svm model
def SVM_training(X_train,y_train,X_test,data_set,RBF,no):
  print("SVM " + str(no) +": " + "RBF = " + str(RBF) + ", predict on " + str(data_set))
  clf = OneVsRestClassifier(SVC(kernel ='rbf',C = RBF,gamma= 'auto'))
  # train the model
  train=clf.fit(X_train,y_train)
  # predict the target
  y_Pred= train.predict(X_test)
  pred =pd.DataFrame(y_Pred, columns =['Dos','Normal', 'Probe', 'R2L','U2R'])
  return pred

#predict
# X --> sample
# y --> target
# 6 kinds of RBF parameter (5, 2, 1, 0.5, 0.2, 6)
pred1 = SVM_training(X,y,x1_test,data_set = 'data 1',RBF = 5,no =1)
pred2 = SVM_training(X,y,x2_test,data_set = 'data 2',RBF = 2,no =2)
pred3 = SVM_training(X,y,x3_test,data_set = 'data 3',RBF = 1,no =3)
pred4 = SVM_training(X,y,x4_test,data_set = 'data 4',RBF = 0.5,no =4)
pred5 = SVM_training(X,y,x5_test,data_set = 'data 5',RBF = 0.2,no =5)
pred6 = SVM_training(X,y,x6_test,data_set = 'data 6',RBF = 0.1,no =6)

"""**claculate the prediction accuracy**"""

# accuracy
def SVM_accuracy(real, pred, data_set):
  score = []
  for i in pred:
    score.append(accuracy_score(real[i],pred[i]))
  accuracy = {'Dos':score[0]*100,'Normal':score[1]*100,'Probe':score[2]*100,
          'R2L':score[3]*100,'U2R':score[4]*100}
  return np.array(list(accuracy.values()))

# calculate the average accuracy
accuracy1 =SVM_accuracy(y1_test, pred1, "data 1")
accuracy2 =SVM_accuracy(y2_test, pred2, "data 2")
accuracy3 =SVM_accuracy(y3_test, pred3, "data 3")
accuracy4 =SVM_accuracy(y4_test, pred4, "data 4")
accuracy5 =SVM_accuracy(y5_test, pred5, "data 5")
accuracy6 =SVM_accuracy(y6_test, pred6, "data 6")
avg = (accuracy1+accuracy2+accuracy3+accuracy4+accuracy5+accuracy6)/6
accuracy = [] 
for i in avg:
  accuracy.append(i)
res = {'Dos':accuracy[0],'Normal':accuracy[1],'Probe':accuracy[2],
      'R2L':accuracy[3],'U2R':accuracy[4]}
print(res)

"""------------------------------------------------------------------------------

**------------------>Importing and Using the KNN classifier<----------------**
"""

from sklearn.neighbors import KNeighborsClassifier
def Knn_model(X_train, y_train, X_test, data_name = 'traing set',neighbors = 1,no =1):
  
  k_nn = KNeighborsClassifier(n_neighbors=neighbors, algorithm = 'auto', p=2, metric='euclidean')
  knn = OneVsRestClassifier(k_nn)
  train=knn.fit(X_train,y_train)

  yPred= train.predict(X_test)
  y_Pred =pd.DataFrame(yPred, columns =['Dos','Normal', 'Probe', 'R2L','U2R'])
  print("Knn_{} ,".format(no) +  str(data_name) + " , predict on the expert no {}".format(neighbors))

  print("Knn_{}  ".format(no)  +  " Train on the expert no {}".format(neighbors))
  return y_Pred

"""**------------->Using the training data to Train the KNN model<-------------**

"""

# predict1 = Knn_model(X_train, y_train, x1_test, data_name = 'Data1',neighbors = 1,no =1)
predict1 = Knn_model(X, y, x1_test, data_name = 'Data1',neighbors = 1,no =1)

"""**------------->Using the other parts of the training data to Train the KNN model<-------------**"""

predict2 = Knn_model(X, y, x2_test, data_name = 'Data2',neighbors = 3,no =2)
predict3 = Knn_model(X, y, x3_test, data_name = 'Data3',neighbors = 5,no =3)
predict4 = Knn_model(X, y, x4_test, data_name = 'Data4',neighbors = 7,no =4)
predict5 = Knn_model(X, y, x5_test, data_name = 'Data5',neighbors = 9,no =5)
predict6 = Knn_model(X, y, x6_test, data_name = 'Data6',neighbors = 11,no =6)

"""**------------------>A function to calculate the accuracy of with respect to model and test data<------------------**"""

def KNN_accuracy(test_data, prediction, dataset_name):
  score = list()
  for i in prediction:
    score.append(accuracy_score(test_data[i],prediction[i]))
  knn_accuracy = {'Dos':score[0]*100,'Normal':score[1]*100,'Probe':score[2]*100,
          'R2L':score[3]*100,'U2R':score[4]*100}
  return np.array(list(knn_accuracy.values()))

"""**----------------->Using the accuracy function for score prediction<-------------------**"""

knn_pred_score1 = KNN_accuracy(y1_test, predict1, dataset_name = 'test set 1')
knn_pred_score2 = KNN_accuracy(y2_test, predict2, dataset_name = 'test set 2')
knn_pred_score3 = KNN_accuracy(y3_test, predict3, dataset_name = 'test set 3')
knn_pred_score4 = KNN_accuracy(y4_test, predict4, dataset_name = 'test set 4')
knn_pred_score5 = KNN_accuracy(y5_test, predict5, dataset_name = 'test set 5')
knn_pred_score6 = KNN_accuracy(y6_test, predict6, dataset_name = 'test set 6')

"""**-------------------->Grouping and finding the average KNN scores<------------------------**"""

knn_avg_pred_score = (knn_pred_score1+knn_pred_score2+knn_pred_score3+knn_pred_score4+knn_pred_score5+knn_pred_score6)/6
knn_accuracy = [] 
for i in knn_avg_pred_score:
  knn_accuracy.append(i)
knn_res = {'Dos':knn_accuracy[0],'Normal':knn_accuracy[1],'Probe':knn_accuracy[2],
      'R2L':knn_accuracy[3],'U2R':knn_accuracy[4]}
print(knn_res)

"""**---------------->Performing the cross validation and determining the accuracy of the model<----------------**"""

from sklearn.model_selection import cross_val_score
neighbors =[1,3,5,7,9,11]
myScore = []
for neighbour in neighbors:
  k_nn = KNeighborsClassifier(n_neighbors=neighbour, algorithm = 'auto', p=2, metric='euclidean')
  knn = OneVsRestClassifier(k_nn)
  score = cross_val_score(knn, x7_valid, y7_valid,cv =10 )
  myScore.append(score)

knn_accuracy = np.array(myScore).mean()
print("Knn model acuuracy: {}%". format(int(round(knn_accuracy*100))))

"""**-------------------->Creating the ensemble method<----------------------**"""

from sklearn.ensemble import VotingClassifier
from sklearn.pipeline import Pipeline

x_max = 10 * np.ones(5)
x_min = -1 * x_max
bounds = (x_min, x_max)

# Commented out IPython magic to ensure Python compatibility.
! pip install pyswarms
import pyswarms as ps
from pyswarms.utils.functions import single_obj as fx
# %load_ext autoreload
# %autoreload 2
options = {'c1': 0.8, 'c2': 0.6, 'w':0.9}
optimizer = ps.single.GlobalBestPSO(n_particles=x7_valid.shape[0],dimensions=x7_valid.shape[1], options=options)
cost, pos = optimizer.optimize(fx.sphere, iters=1000)

estimators = []
leng = [1,2,3,4,5,6]
for n,No in zip(([1,3,5,7,9,11]),leng):
  KNN = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=n, algorithm = 'auto', p=2, metric='euclidean'))
  estimators.append(("Knn_{} ,".format(No),KNN))

leng = [1,2,3,4,5,6]
for c,No in zip(([5,2,1,0.5,0.2,0.1]),leng):
  SVM = OneVsRestClassifier(SVC(kernel = 'rbf',C = c,gamma= 'auto',probability=True))
  estimators.append(("SVM_{} ,".format(No),SVM))

weight=list(pos[0:12])
svm_knn =OneVsRestClassifier(VotingClassifier(estimators = estimators, voting='soft',weights=weight))
svm_knn = svm_knn.fit(X.values,y.values)
yPred1 =  svm_knn.predict(x1_test)

yPred2=  svm_knn.predict(x2_test)
yPred3=  svm_knn.predict(x3_test)
yPred4=  svm_knn.predict(x4_test)
yPred5=  svm_knn.predict(x5_test)
yPred6=  svm_knn.predict(x6_test)

def PSO_accuracy(test_data, prediction, dataset_name):
  score = list()
  for i in prediction:
    score.append(accuracy_score(test_data[i],prediction[i]))
  pso_accuracy = {'Dos':score[0]*100,'Normal':score[1]*100,'Probe':score[2]*100,
          'R2L':score[3]*100,'U2R':score[4]*100}
  return np.array(list(pso_accuracy.values()))

pso_pred_score1 = PSO_accuracy(y1_test.values, yPred1, dataset_name = 'test set 1')
pso_pred_score2 = PSO_accuracy(y2_test.values, yPred2, dataset_name = 'test set 2')
pso_pred_score3 = PSO_accuracy(y3_test.values, yPred3, dataset_name = 'test set 3')
pso_pred_score4 = PSO_accuracy(y4_test.values, yPred4, dataset_name = 'test set 4')
pso_pred_score5 = PSO_accuracy(y5_test.values, yPred5, dataset_name = 'test set 5')
pso_pred_score6 = PSO_accuracy(y6_test.values, yPred6, dataset_name = 'test set 6')

pso_avg_pred_score = (pso_pred_score1+pso_pred_score2+pso_pred_score3+pso_pred_score4+pso_pred_score5+pso_pred_score6)/6
pso_accuracy = [] 
for i in pso_avg_pred_score:
  pso_accuracy.append(i)
pso_res = {'Dos':pso_accuracy[0],'Normal':pso_accuracy[1],'Probe':pso_accuracy[2],
      'R2L':pso_accuracy[3],'U2R':pso_accuracy[4]}
print(pso_res)